{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "78a858d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import chromadb\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f37fc2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0a9022f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/gemini-embedding-001\n"
     ]
    }
   ],
   "source": [
    "for m in genai.list_models():\n",
    "    if 'embedContent' in m.supported_generation_methods:\n",
    "        print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f2f64205",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"./document/british_gp_2020.docx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "89cf82f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total structured chunks: 12\n",
      "\n",
      "--- Chunk 1 (section) ---\n",
      "Heading: None\n",
      "Content preview: British Grand Prix 2020 — Silverstone Circuit Track Reference\n",
      "Purpose: Canonical track facts + commentary cues for an LLM-powered race commentary bot.\n",
      "Last updated: 02 January 2026...\n",
      "\n",
      "--- Chunk 2 (table) ---\n",
      "Heading: 1) Quick facts (Silverstone Grand Prix configuration used in 2020)\n",
      "Content preview: Field | Value\n",
      "Circuit name | Silverstone Circuit (Grand Prix / 'Arena' layout used since 2011)\n",
      "Location | Silverstone, Northamptonshire, England, United Kingdom\n",
      "Direction | Clockwise\n",
      "FIA licence grade...\n",
      "\n",
      "--- Chunk 3 (section) ---\n",
      "Heading: 2) Layout overview\n",
      "Content preview: Silverstone is one of the fastest circuits on the F1 calendar, built on a former RAF airfield. It combines long full-throttle sections with sustained high-speed direction changes that generate very hi...\n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "from docx.table import Table\n",
    "from docx.text.paragraph import Paragraph\n",
    "\n",
    "doc = Document(filepath)\n",
    "\n",
    "def iter_block_items(parent):\n",
    "    \"\"\"Yield paragraphs and tables in document order.\"\"\"\n",
    "    body = parent._element.body\n",
    "    for child in body.iterchildren():\n",
    "        if child.tag.endswith('p'):\n",
    "            yield Paragraph(child, parent)\n",
    "        elif child.tag.endswith('tbl'):\n",
    "            yield Table(child, parent)\n",
    "\n",
    "def get_heading_level(paragraph):\n",
    "    \"\"\"Get heading level from paragraph style (1-9 for headings, None for body text).\"\"\"\n",
    "    style_name = paragraph.style.name if paragraph.style else \"\"\n",
    "    if style_name.startswith(\"Heading\"):\n",
    "        try:\n",
    "            return int(style_name.split()[-1])\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "# Extract structured chunks based on document structure\n",
    "structured_chunks = []\n",
    "current_section = {\"heading\": None, \"content\": []}\n",
    "\n",
    "for block in iter_block_items(doc):\n",
    "    if isinstance(block, Paragraph):\n",
    "        text = block.text.strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        \n",
    "        heading_level = get_heading_level(block)\n",
    "        \n",
    "        if heading_level is not None:\n",
    "            # Save previous section if it has content\n",
    "            if current_section[\"content\"]:\n",
    "                structured_chunks.append({\n",
    "                    \"heading\": current_section[\"heading\"],\n",
    "                    \"content\": \"\\n\".join(current_section[\"content\"]),\n",
    "                    \"type\": \"section\"\n",
    "                })\n",
    "            # Start new section\n",
    "            current_section = {\"heading\": text, \"content\": []}\n",
    "        else:\n",
    "            current_section[\"content\"].append(text)\n",
    "            \n",
    "    elif isinstance(block, Table):\n",
    "        # Tables are kept as separate chunks\n",
    "        table_rows = []\n",
    "        for row in block.rows:\n",
    "            cells = [cell.text.strip().replace(\"\\n\", \" \") for cell in row.cells]\n",
    "            if any(cells):\n",
    "                table_rows.append(\" | \".join(cells))\n",
    "        \n",
    "        if table_rows:\n",
    "            table_text = \"\\n\".join(table_rows)\n",
    "            structured_chunks.append({\n",
    "                \"heading\": current_section[\"heading\"],\n",
    "                \"content\": table_text,\n",
    "                \"type\": \"table\"\n",
    "            })\n",
    "\n",
    "# Don't forget the last section\n",
    "if current_section[\"content\"]:\n",
    "    structured_chunks.append({\n",
    "        \"heading\": current_section[\"heading\"],\n",
    "        \"content\": \"\\n\".join(current_section[\"content\"]),\n",
    "        \"type\": \"section\"\n",
    "    })\n",
    "\n",
    "print(f\"Total structured chunks: {len(structured_chunks)}\")\n",
    "for i, chunk in enumerate(structured_chunks[:3]):\n",
    "    print(f\"\\n--- Chunk {i+1} ({chunk['type']}) ---\")\n",
    "    print(f\"Heading: {chunk['heading']}\")\n",
    "    print(f\"Content preview: {chunk['content'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ba6654cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final chunks after splitting large sections: 15\n"
     ]
    }
   ],
   "source": [
    "# Document Structure-based Chunking\n",
    "# Optionally split large sections while preserving structure\n",
    "\n",
    "MAX_CHUNK_SIZE = 1000  # Maximum characters per chunk\n",
    "\n",
    "def split_large_chunk(chunk, max_size=MAX_CHUNK_SIZE):\n",
    "    \"\"\"Split large chunks while trying to preserve paragraph boundaries.\"\"\"\n",
    "    content = chunk[\"content\"]\n",
    "    if len(content) <= max_size:\n",
    "        return [chunk]\n",
    "    \n",
    "    # Split by paragraphs first\n",
    "    paragraphs = content.split(\"\\n\")\n",
    "    sub_chunks = []\n",
    "    current_content = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        para_len = len(para) + 1  # +1 for newline\n",
    "        if current_length + para_len > max_size and current_content:\n",
    "            sub_chunks.append({\n",
    "                \"heading\": chunk[\"heading\"],\n",
    "                \"content\": \"\\n\".join(current_content),\n",
    "                \"type\": chunk[\"type\"]\n",
    "            })\n",
    "            current_content = [para]\n",
    "            current_length = para_len\n",
    "        else:\n",
    "            current_content.append(para)\n",
    "            current_length += para_len\n",
    "    \n",
    "    if current_content:\n",
    "        sub_chunks.append({\n",
    "            \"heading\": chunk[\"heading\"],\n",
    "            \"content\": \"\\n\".join(current_content),\n",
    "            \"type\": chunk[\"type\"]\n",
    "        })\n",
    "    \n",
    "    return sub_chunks\n",
    "\n",
    "# Process all chunks\n",
    "final_chunks = []\n",
    "for chunk in structured_chunks:\n",
    "    final_chunks.extend(split_large_chunk(chunk))\n",
    "\n",
    "print(f\"Final chunks after splitting large sections: {len(final_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2a657257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 15 LangChain documents\n",
      "\n",
      "--- First Document ---\n",
      "('British Grand Prix 2020 — Silverstone Circuit Track Reference\\n'\n",
      " 'Purpose: Canonical track facts + commentary cues for an LLM-powered race '\n",
      " 'commentary bot.\\n'\n",
      " 'Last updated: 02 January 2026')\n",
      "\n",
      "Metadata: {'heading': None, 'chunk_type': 'section', 'chunk_index': 0}\n"
     ]
    }
   ],
   "source": [
    "# Convert to LangChain Document format for compatibility with downstream processing\n",
    "from langchain_core.documents import Document as LangchainDoc\n",
    "\n",
    "texts = []\n",
    "for i, chunk in enumerate(final_chunks):\n",
    "    # Create metadata with structural information\n",
    "    metadata = {\n",
    "        \"heading\": chunk[\"heading\"],\n",
    "        \"chunk_type\": chunk[\"type\"],\n",
    "        \"chunk_index\": i\n",
    "    }\n",
    "    doc = LangchainDoc(page_content=chunk[\"content\"], metadata=metadata)\n",
    "    texts.append(doc)\n",
    "\n",
    "print(f\"Created {len(texts)} LangChain documents\\n\")\n",
    "print(\"--- First Document ---\")\n",
    "pprint(texts[0].page_content)\n",
    "print(f\"\\nMetadata: {texts[0].metadata}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "f1bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
